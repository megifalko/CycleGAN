{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions to allow image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image, width, height):\n",
    "  cropped_image = tf.image.random_crop(\n",
    "      image, size=[height, width, 3])\n",
    "\n",
    "  return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_jitter(image, width, height):\n",
    "  # resizing to 286 x 286 x 3\n",
    "  image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "  # randomly cropping to 256 x 256 x 3\n",
    "  image = random_crop(image, width, height)\n",
    "\n",
    "  # random mirroring\n",
    "  image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_norm_layer(norm):\n",
    "    if norm == 'none':\n",
    "        return lambda: lambda x: x\n",
    "    elif norm == 'batch_norm':\n",
    "        return keras.layers.BatchNormalization\n",
    "    elif norm == 'instance_norm':\n",
    "        return tfa.layers.InstanceNormalization\n",
    "    elif norm == 'layer_norm':\n",
    "        return keras.layers.LayerNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator is build using the ResNet architecture. ResNet stands for Residual Network and is a deep learning model which relieves the problem of vanishing gradient in deep networks. It introduces skip connections which allow alternate shortcut path for the gradient to flow through. Moreover, if a particular layer hurts the performance of the ResNet, it will be skipped by regularization.\n",
    "\n",
    "Building block of a ResNet is called a residual block. Activation of a residual block layer is fast-forwarded to a deeper layer in the neural network.\n",
    "\n",
    "An image convolution is an element-wise multiplication of two matrices followed by a summation of obtained elements. Image is a multi-dimensional matrix which has a certain width and height. It also has a depth of 3 - one channel for each color - red, green or blue.\n",
    "\n",
    "Kernel is a smaller matrix which sits on top of the image and slides from left-to-right and top-to-bottom applying a convolution at each coordinate of the original image - kernel stops at each location, examines the neighborhood of pixels located at the center, convolution is performed and output value (kernel output) stored in the output image at the same coordinates as the center of the kernel. Odd kernel size is used to ensure there is a valid integer coordinate at the center.\n",
    "\n",
    "Pixels located on the border of the image are not in the center of any sliding window - this implies there is a decrease in spatial dimension of the image. To ensure that output image has the same dimensions as input image, padding is applied. Reflect mode of padding reuses the contents of current row or column for padding the values. It duplicates the image values along the borders but in reverse order, hence the name 'reflect mode'.\n",
    "\n",
    "*keras.layers.Conv2D* creates a convolution kernel which is later convolved with the input layer to produce a tensor of outputs. The first parameter to this function is the number of filters that the convolutional layer will learn. The second argument is the kernel size. Padding 'valid' means no padding and as a result spatial dimansions are allowed to reduce. Padding 'same' is used to indicate that the dimensions of the input image should be preserved. The addition od a bias vector is controlled by use_bias argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResnetGenerator(input_shape=(256, 256, 3),\n",
    "                    output_channels=3,\n",
    "                    dim=64,\n",
    "                    n_downsamplings=2,\n",
    "                    n_blocks=9,\n",
    "                    norm='instance_norm'):\n",
    "    Norm = _get_norm_layer(norm)\n",
    "\n",
    "    def _residual_block(x):\n",
    "        dim = x.shape[-1]\n",
    "        print(dim)\n",
    "        h = x\n",
    "\n",
    "        h = tf.pad(h, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n",
    "        h = keras.layers.Conv2D(dim, 3, padding='valid', use_bias=False)(h)\n",
    "        h = Norm()(h)\n",
    "        h = tf.nn.relu(h)\n",
    "\n",
    "        h = tf.pad(h, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n",
    "        h = keras.layers.Conv2D(dim, 3, padding='valid', use_bias=False)(h)\n",
    "        h = Norm()(h)\n",
    "\n",
    "        return keras.layers.add([x, h])\n",
    "\n",
    "    # 0\n",
    "    h = inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # 1\n",
    "    h = tf.pad(h, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n",
    "    h = keras.layers.Conv2D(dim, 7, padding='valid', use_bias=False)(h)\n",
    "    h = Norm()(h)\n",
    "    h = tf.nn.relu(h)\n",
    "\n",
    "    # 2\n",
    "    for _ in range(n_downsamplings):\n",
    "        dim *= 2\n",
    "        h = keras.layers.Conv2D(dim, 3, strides=2, padding='same', use_bias=False)(h)\n",
    "        h = Norm()(h)\n",
    "        h = tf.nn.relu(h)\n",
    "\n",
    "    # 3\n",
    "    for _ in range(n_blocks):\n",
    "        h = _residual_block(h)\n",
    "\n",
    "    # 4\n",
    "    for _ in range(n_downsamplings):\n",
    "        dim //= 2\n",
    "        h = keras.layers.Conv2DTranspose(dim, 3, strides=2, padding='same', use_bias=False)(h)\n",
    "        h = Norm()(h)\n",
    "        h = tf.nn.relu(h)\n",
    "\n",
    "    # 5\n",
    "    h = tf.pad(h, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n",
    "    h = keras.layers.Conv2D(output_channels, 7, padding='valid')(h)\n",
    "    h = tf.tanh(h)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator uses *leaky_relu* function which stands for retified linear unit activation. It is commonly used activation function which speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvDiscriminator(input_shape=(256, 256, 3),\n",
    "                      dim=64,\n",
    "                      n_downsamplings=3,\n",
    "                      norm='instance_norm'):\n",
    "    dim_ = dim\n",
    "    Norm = _get_norm_layer(norm)\n",
    "\n",
    "    # 0\n",
    "    h = inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # 1\n",
    "    h = keras.layers.Conv2D(dim, 4, strides=2, padding='same')(h)\n",
    "    h = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "\n",
    "    for _ in range(n_downsamplings - 1):\n",
    "        dim = min(dim * 2, dim_ * 8)\n",
    "        h = keras.layers.Conv2D(dim, 4, strides=2, padding='same', use_bias=False)(h)\n",
    "        h = Norm()(h)\n",
    "        h = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "\n",
    "    # 2\n",
    "    dim = min(dim * 2, dim_ * 8)\n",
    "    h = keras.layers.Conv2D(dim, 4, strides=1, padding='same', use_bias=False)(h)\n",
    "    h = Norm()(h)\n",
    "    h = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "\n",
    "    # 3\n",
    "    h = keras.layers.Conv2D(1, 4, strides=1, padding='same')(h)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of loss functions is to compute the quantity that a model should seek to minimize during training. *tf.keras.losses.BinaryCrossentropy* computes the cross-entropy loss between true labels and predicted labels. Cross entropy loss is a metric used to measure how well a classification model in machine learning performs. Cross-entropy loss increases as the predicted probability diverges from the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CycleGAN, there is no paired data to train on, hence there is no guarantee that the input x and the target y pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the cycle consistency loss is calculated. Cycle consistency means the result should be as close to the original input as possible.\n",
    "\n",
    "Identity loss is added to help preserve tint. It states that when given an image of the target class, a generator should return that same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "  real_loss = loss_obj(tf.ones_like(real), real)\n",
    "\n",
    "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(generated):\n",
    "  return loss_obj(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "  return LAMBDA * loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(real_image, same_image):\n",
    "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "  return LAMBDA * 0.5 * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and plot images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, epoch = ''):\n",
    "  prediction = model(test_input)\n",
    "\n",
    "  fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "  display_list = [test_input[0], prediction[0]]\n",
    "  title = ['Input Image', 'Predicted Image']\n",
    "  if(epoch):\n",
    "      title[1] = title[1] + ', epoch = ' + str(epoch)\n",
    "\n",
    "  for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot it.\n",
    "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "  plt.savefig('output/' + ''.join(random.choices(string.ascii_lowercase + string.digits, k=10)) + '.png', bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a491fa038a24a3354ef15b8320e5eed1f98c46448a463343d2ea596d5b86218b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
